{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import build_model\n",
    "import load_data\n",
    "import vectorize_data\n",
    "import explore_data\n",
    "import train_fine_tuned_sequence_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOAD_DATA'''\n",
    "\n",
    "import csv\n",
    "text = []\n",
    "label = []\n",
    "data = []\n",
    "with open('../clean.csv') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    for row in reader:\n",
    "        text.append(row[0])\n",
    "        label.append(row[1])\n",
    "        data.append(row)\n",
    "\n",
    "text = text[1:]\n",
    "label = label[1:]\n",
    "label = list(map(int, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, \n",
    "                                                    label, \n",
    "                                                    test_size = 0.1, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# data = train_test_split(text, label, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "\n",
    "def _data_generator(x, y, num_features, batch_size):\n",
    "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
    "    # Arguments\n",
    "        x: np.matrix, feature matrix.\n",
    "        y: np.ndarray, labels.\n",
    "        num_features: int, number of features.\n",
    "        batch_size: int, number of samples per batch.\n",
    "    # Returns\n",
    "        Yields feature and label data in batches.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            if end_idx > num_samples:\n",
    "                end_idx = num_samples\n",
    "            x_batch = x[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get test data'''\n",
    "\n",
    "def preprocess(path):\n",
    "\t\n",
    "\tf = open(path, 'r')\n",
    "\tlines = f.readlines()\n",
    "\n",
    "\ttemp_data = []\n",
    "\tid = []\n",
    "\ttext = []\n",
    "\tfor l in lines:\n",
    "\t\ttemp = l.split('|')\n",
    "\t\ttemp[1] = ''.join([c for c in temp[1] if c.isalpha() or c == ' '])\n",
    "\t\ttemp_data.append(temp)\n",
    "\n",
    "\t\tid.append(temp[0])\n",
    "\t\ttext.append(temp[1])\n",
    "\n",
    "\t# print(temp_data)\n",
    "\treturn temp_data, id, text\n",
    "\n",
    "data, id, val_text = preprocess('../query_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train_sequence_model(data,\n",
    "                               learning_rate=1e-3,\n",
    "                               epochs=10,\n",
    "                               batch_size=64,\n",
    "                               blocks=2,\n",
    "                               filters=64,\n",
    "                               dropout_rate=0.2,\n",
    "                               embedding_dim=200,\n",
    "                               kernel_size=3,\n",
    "                               pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = explore_data.get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index, tokenizer, max_length = vectorize_data.sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "    \n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "    \n",
    "    ''' '''\n",
    "    embedding_data_dir = 'glove.6B'\n",
    "    embedding_matrix = train_fine_tuned_sequence_model._get_embedding_matrix(word_index, embedding_data_dir, embedding_dim)\n",
    "    ''' '''\n",
    "    \n",
    "    # Create model instance.\n",
    "    model = build_model.sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2), \n",
    "                 tf.keras.callbacks.ModelCheckpoint('.mdl_wts.hdf5',\n",
    "                                                    save_best_only=True, monitor='val_loss', mode = 'min')]\n",
    "\n",
    "    # Create training and validation generators.\n",
    "    training_generator = _data_generator(\n",
    "        x_train, train_labels, num_features, batch_size)\n",
    "    validation_generator = _data_generator(\n",
    "        x_val, val_labels, num_features, batch_size)\n",
    "\n",
    "    # Get number of training steps. This indicated the number of steps it takes\n",
    "    # to cover all samples in one epoch.\n",
    "    steps_per_epoch = x_train.shape[0] // batch_size\n",
    "    if x_train.shape[0] % batch_size:\n",
    "        steps_per_epoch += 1\n",
    "\n",
    "    # Get number of validation steps.\n",
    "    validation_steps = x_val.shape[0] // batch_size\n",
    "    if x_val.shape[0] % batch_size:\n",
    "        validation_steps += 1\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit_generator(\n",
    "            generator=training_generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            epochs=epochs,\n",
    "            verbose=1)  # Logs once per epoch.\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('amazon_reviews_sepcnn_model.h5')\n",
    "    # return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "    return tokenizer, max_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1045/1415 [=====================>........] - ETA: 2:28 - loss: 1.4142 - acc: 0.5503"
     ]
    }
   ],
   "source": [
    "data = (x_train, y_train), (x_test, y_test)\n",
    "tokenizer, max_length = batch_train_sequence_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = tokenizer.texts_to_sequences(val_text)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/media/vdev/Paradise/Project/Hackathon/SepCNN/amazon_reviews_sepcnn_model.h5')\n",
    "pred = model.predict(x_val)\n",
    "preds = np.argmax(pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "sub = list(zip(id, preds))\n",
    "with open('submit1.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
